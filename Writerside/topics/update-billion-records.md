# レコード数が多い場合の更新

50~60億レコード
変更内容はデータを埋めるだけ

今までのやり方でどのくらい時間がかかるか計測
-> 1000件で70分

snowflake idを使用しているprimary keyがある
-> uuid, timestampを元に生成される

時間が最近になればなるほど、データが増える
-> ユーザーが増えるので

sidekiq worker内で非同期jobを実行する

小さいデータセットから始める
dead lockが発生する可能性あり。(それはそう)
